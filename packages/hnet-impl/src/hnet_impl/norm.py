import math
import torch
import triton
import triton.language as tl


# Autotune for warp counts which are powers of 2 and do not exceed thread per block limit
def triton_autotune_configs(warp_size=32, max_threads_per_block=1024):
    return [
        triton.Config({}, num_warps=warp_count)
        for warp_count in [1, 2, 4, 8, 16, 32]
        if warp_count * warp_size <= max_threads_per_block
    ]


@triton.autotune(configs=triton_autotune_configs(), key=["N"])
@triton.jit
def _rms_norm_fwd_kernel(
    X,  # *in*  bf16  [M, N]
    RES,  # *in*  fp32  [M, N]
    WEIGHT,  # *in*  fp32  [N]
    Y_OUT,  # *out* bf16  [M, N]  (weighted x_hat)
    RES_OUT,  # *out* fp32  [M, N]  (combined pre-norm)
    M,
    N,  # ints
    EPS: tl.constexpr,  # float (compile-time)
    ALPHA_X: tl.constexpr,  # float (compile-time)
    ALPHA_RES: tl.constexpr,  # float (compile-time)
    BLOCK_N: tl.constexpr,  # power-of-two tile
):
    row = tl.program_id(0)
    cols = tl.arange(0, BLOCK_N)
    mask = cols < N

    # presumed stride always is N
    X += row * N
    RES += row * N
    Y_OUT += row * N
    RES_OUT += row * N

    # always load to fp32
    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)
    res = tl.load(RES + cols, mask=mask, other=0.0).to(tl.float32)
    w = tl.load(WEIGHT + cols, mask=mask, other=0.0).to(tl.float32)

    x_comb = ALPHA_X * x + ALPHA_RES * res

    # RMS stats
    mean_sq = tl.sum(x_comb * x_comb, axis=0) / N
    rstd = 1.0 / tl.sqrt(mean_sq + EPS)
    x_hat = x_comb * rstd

    y = (x_hat * w).to(tl.bfloat16)

    tl.store(Y_OUT + cols, y, mask=mask)
    tl.store(RES_OUT + cols, x_comb, mask=mask)


@triton.autotune(configs=triton_autotune_configs(), key=["N"])
@triton.jit
def _rms_norm_bwd_kernel(
    RES_OUT,  # *in*  fp32  [M, N]  (from forward; combined input)
    DRES_OUT,  # *in*  fp32  [M, N]  (upstream grad wrt res_out)
    DY,  # *in*  bf16  [M, N]
    WEIGHT,  # *in*  fp32  [N]
    DX,  # *out* bf16  [M, N]
    DRES,  # *out* fp32  [M, N]
    DW_PARTIAL,  # *out* fp32  [M, N]
    M: int,
    N: int,
    EPS: tl.constexpr,
    ALPHA_X: tl.constexpr,
    ALPHA_RES: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    row = tl.program_id(0)
    cols = tl.arange(0, BLOCK_N)
    mask = cols < N

    # presumed stride is always N
    RES_OUT += row * N
    DRES_OUT += row * N
    DY += row * N
    DX += row * N
    DRES += row * N
    DW_PARTIAL += row * N

    # Loads (fp32 math)
    d_res_out = tl.load(DRES_OUT + cols, mask=mask, other=0.0).to(tl.float32)
    x_comb = tl.load(RES_OUT + cols, mask=mask, other=0.0).to(tl.float32)
    dy = tl.load(DY + cols, mask=mask, other=0.0).to(tl.float32)
    w = tl.load(WEIGHT + cols, mask=mask, other=0.0).to(tl.float32)

    # Recompute RMS stats for this row
    mean_sq = tl.sum(x_comb * x_comb, axis=0) / N
    rstd = 1.0 / tl.sqrt(mean_sq + EPS)
    x_hat = x_comb * rstd

    # Backprop (y = w * x_hat)
    d_xhat = dy * w
    c1 = tl.sum(x_hat * d_xhat, axis=0) / N
    d_xcomb_from_y = (d_xhat - x_hat * c1) * rstd
    d_xcomb = d_xcomb_from_y + d_res_out

    dx = (ALPHA_X * d_xcomb).to(tl.bfloat16)
    dres = ALPHA_RES * d_xcomb

    tl.store(DX + cols, dx, mask=mask)
    tl.store(DRES + cols, dres, mask=mask)

    # Perâ€‘row partial for dweight: dy * x_hat
    dw_row = dy * x_hat
    tl.store(DW_PARTIAL + cols, dw_row, mask=mask)


def get_block_n(n: int, esize: int):
    # Less than 64KB per feature: enqueue fused kerne>l
    block_n = min(65536 // esize, triton.next_power_of_2(n))
    assert n <= block_n, "This RMSNorm doesn't support feature dim >= 64KB."
    return block_n


class _RMSNormBF16FP32Fn(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx,
        x_bf16: torch.Tensor,
        residual_f32: torch.Tensor,
        weight: torch.Tensor,
        eps: float,
        alpha_x: float,
        alpha_res: float,
    ):
        x_shape = x_bf16.shape
        N = x_shape[-1]
        M = math.prod(x_shape[:-1])
        # shape & dtype checks
        assert x_bf16.dtype == torch.bfloat16, "x must be bfloat16"
        assert residual_f32.dtype == torch.float32, "residual must be float32"
        assert x_bf16.shape == residual_f32.shape and x_bf16.ndim >= 2
        assert weight.ndim == 1 and weight.shape[0] == N, (
            "weight must be 1D contiguous with length N"
        )

        # Collapse to 2D [M, N] with contiguous last dim
        x2 = x_bf16.view(M, N).contiguous()
        res2 = residual_f32.view(M, N).contiguous()
        assert x2.stride(-1) == 1 and res2.stride(-1) == 1, (
            "last dim must be contiguous"
        )

        # Allocate outputs
        y2 = torch.empty_like(x2, dtype=torch.bfloat16)
        res_out = torch.empty_like(res2, dtype=torch.float32)

        _rms_norm_fwd_kernel[(M,)](
            x2,
            res2,
            weight,
            y2,
            res_out,
            M,
            N,
            EPS=float(eps),
            ALPHA_X=float(alpha_x),
            ALPHA_RES=float(alpha_res),
            BLOCK_N=get_block_n(N, x_bf16.element_size()),
        )

        # Save for backward
        ctx.eps = float(eps)
        ctx.alpha_x = float(alpha_x)
        ctx.alpha_res = float(alpha_res)
        ctx.save_for_backward(res_out, weight)
        ctx.shape = x_shape

        return y2.view(x_shape), res_out

    @staticmethod
    def backward(ctx, dy_bf16: torch.Tensor, dres_out_f32: torch.Tensor):
        assert dy_bf16.dtype == torch.bfloat16, "grad_out must be bfloat16"
        assert dres_out_f32.dtype == torch.float32

        (res_out, weight) = ctx.saved_tensors
        x_shape = ctx.shape
        M = math.prod(x_shape[:-1])
        N = x_shape[-1]

        dy2 = dy_bf16.view(M, N).contiguous()
        res_out2 = res_out.view(M, N).contiguous()
        dres_out2 = dres_out_f32.view(M, N).contiguous().to(torch.float32)
        assert dy2.stride(-1) == 1 == res_out2.stride(-1) == dres_out2.stride(-1)

        dx2 = torch.empty_like(dy2, dtype=torch.bfloat16)
        dres2 = torch.empty_like(res_out2, dtype=torch.float32)
        dweight_partials = torch.empty(
            (M, N), dtype=torch.float32, device=weight.device
        )

        _rms_norm_bwd_kernel[(M,)](
            res_out2,
            dres_out2,
            dy2,
            weight,
            dx2,
            dres2,
            dweight_partials,
            M,
            N,
            EPS=ctx.eps,
            ALPHA_X=ctx.alpha_x,
            ALPHA_RES=ctx.alpha_res,
            BLOCK_N=get_block_n(N, dy_bf16.element_size()),
        )

        # Reduce partials across rows to get [N]
        dweight = dweight_partials.sum(dim=0).to(weight.dtype)

        # Grads for eps/alpha_x/alpha_res are None
        return dx2.view(x_shape), dres2.view(x_shape), dweight, None, None, None


def fused_rmsnorm_with_residual(
    x_bf16: torch.Tensor,
    residual_f32: torch.Tensor,
    weight: torch.Tensor,
    eps: float,
    alpha_x: float,
    alpha_res: float,
):
    """
    y = weight * (alpha_x * x + alpha_res * residual) / sqrt(mean((alpha_x*x + alpha_res*residual)^2) + eps)
    - x_bf16:     bfloat16 tensor [..., N]
    - residual:   float32 tensor [..., N]
    - weight:     float32 tensor [N] (required)
    - eps, alpha_x, alpha_res: Python floats (constexpr to Triton)
    Returns y in bfloat16. Autograd yields dx in bfloat16, dres/dweight in float32.
    """
    return _RMSNormBF16FP32Fn.apply(
        x_bf16, residual_f32, weight, eps, alpha_x, alpha_res
    )


def rmsnorm_with_residual_native(x_bf16, residual_f32, weight, eps, alpha_x, alpha_res):
    comb = alpha_x * x_bf16.float() + alpha_res * residual_f32
    y32 = torch.nn.functional.rms_norm(comb, (weight.shape[0],), weight, eps)
    return y32.to(torch.bfloat16), comb


def compare_fused_vs_native(x, res, w, ax, ar, eps=1e-5):
    # Fused
    y_fused, r_fused = fused_rmsnorm_with_residual(x, res, w, eps, ax, ar)
    g_y = torch.randn_like(y_fused)
    g_r = torch.randn_like(r_fused)
    dx_fused, dres_fused, dw_fused = torch.autograd.grad(
        outputs=(y_fused, r_fused),
        inputs=(x, res, w),
        grad_outputs=(g_y, g_r),
        retain_graph=False,
        allow_unused=False,
    )

    # Native
    x_bf16_ref = x.detach().clone().requires_grad_(True)
    res_f32_ref = res.detach().clone().requires_grad_(True)
    w_f32_ref = w.detach().clone().requires_grad_(True)

    y_ref, r_ref = rmsnorm_with_residual_native(
        x_bf16_ref, res_f32_ref, w_f32_ref, eps, ax, ar
    )
    dx_ref, dres_ref, dw_ref = torch.autograd.grad(
        outputs=(y_ref, r_ref),
        inputs=(x_bf16_ref, res_f32_ref, w_f32_ref),
        grad_outputs=(g_y, g_r),
    )

    # Compare
    assert torch.allclose(y_fused, y_ref, rtol=3e-2, atol=3e-3), "forward mismatch"
    assert torch.allclose(r_fused, r_ref, rtol=3e-2, atol=3e-3), "forward mismatch"
    assert torch.allclose(dx_ref, dx_fused, rtol=3e-2, atol=3e-3), "dx mismatch"
    assert torch.allclose(dres_ref, dres_fused, rtol=3e-2, atol=3e-3), "dres mismatch"
    assert torch.allclose(dw_ref, dw_fused, rtol=3e-2, atol=3e-3), "dw mismatch"


def test_fused_rmsnorm():
    with torch.random.fork_rng(["cuda"]), torch.device("cuda"):
        torch.manual_seed(0)
        for M, N in [
            (128, 2048),
            (77, 1536),
        ]:
            x_bf16 = torch.randn((M, N), dtype=torch.bfloat16).requires_grad_(True)
            res_f32 = torch.randn((M, N), dtype=torch.float32).requires_grad_(True)
            for w_dtype in [torch.float32, torch.bfloat16]:
                w = torch.randn((N,), dtype=w_dtype).requires_grad_(True)
                for ax, ar in [
                    (1, 1),
                    (0.7, 1.3),
                ]:
                    compare_fused_vs_native(x_bf16, res_f32, w, ax, ar)


__all__ = ["fused_rmsnorm_with_residual"]
if __name__ == "__main__":
    test_fused_rmsnorm()
